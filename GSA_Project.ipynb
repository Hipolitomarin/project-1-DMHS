{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2609b654-4b52-4536-9b15-f75887254519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Type</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>Location</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>...</th>\n",
       "      <th>Species</th>\n",
       "      <th>Source</th>\n",
       "      <th>pdf</th>\n",
       "      <th>href formula</th>\n",
       "      <th>href</th>\n",
       "      <th>Case Number</th>\n",
       "      <th>Case Number.1</th>\n",
       "      <th>original order</th>\n",
       "      <th>Unnamed: 21</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-06-17 00:00:00</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>Hilton Head Island</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>F</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Kevin McMurray Trackingsharks.com:</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-06-11 00:00:00</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Boca Grande</td>\n",
       "      <td>Snorkeling</td>\n",
       "      <td>Leah Lendel</td>\n",
       "      <td>F</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>Bull shark</td>\n",
       "      <td>Kevin McMurray Trackingsharks.com: James Kings...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-05-29 00:00:00</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>Sunset Beach</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Sean Barton</td>\n",
       "      <td>M</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Kevin McMurray Trackingsharks.com: Clay Crewel...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-05-26 00:00:00</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Vanuatu</td>\n",
       "      <td>South Santo</td>\n",
       "      <td>Espiitu Santo Island</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Tumas</td>\n",
       "      <td>M</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Kevin McMurray Trackingsharks.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-05-15 00:00:00</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Australia</td>\n",
       "      <td>South Australia</td>\n",
       "      <td>Port Noarlunga</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Richard Vinall</td>\n",
       "      <td>M</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Simon DeMarchi: Todd Smith: 9 News:ABC News</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Date    Year        Type    Country            State  \\\n",
       "0  2025-06-17 00:00:00  2025.0  Unprovoked        USA   South Carolina   \n",
       "1  2025-06-11 00:00:00  2025.0  Unprovoked        USA          Florida   \n",
       "2  2025-05-29 00:00:00  2025.0  Unprovoked        USA   North Carolina   \n",
       "3  2025-05-26 00:00:00  2025.0  Unprovoked    Vanuatu      South Santo   \n",
       "4  2025-05-15 00:00:00  2025.0  Unprovoked  Australia  South Australia   \n",
       "\n",
       "               Location    Activity            Name Sex Age  ...    Species   \\\n",
       "0  Hilton Head Island      Swimming      Not stated   F  12  ...  Not stated   \n",
       "1           Boca Grande  Snorkeling     Leah Lendel   F   9  ...  Bull shark   \n",
       "2          Sunset Beach    Swimming     Sean Barton   M  26  ...  Not stated   \n",
       "3  Espiitu Santo Island    Swimming           Tumas   M  14  ...  Not stated   \n",
       "4        Port Noarlunga    Swimming  Richard Vinall   M  66  ...  Not stated   \n",
       "\n",
       "                                              Source  pdf href formula href  \\\n",
       "0                Kevin McMurray Trackingsharks.com:   NaN          NaN  NaN   \n",
       "1  Kevin McMurray Trackingsharks.com: James Kings...  NaN          NaN  NaN   \n",
       "2  Kevin McMurray Trackingsharks.com: Clay Crewel...  NaN          NaN  NaN   \n",
       "3                  Kevin McMurray Trackingsharks.com  NaN          NaN  NaN   \n",
       "4        Simon DeMarchi: Todd Smith: 9 News:ABC News  NaN          NaN  NaN   \n",
       "\n",
       "  Case Number Case Number.1 original order Unnamed: 21 Unnamed: 22  \n",
       "0         NaN           NaN            NaN         NaN         NaN  \n",
       "1         NaN           NaN            NaN         NaN         NaN  \n",
       "2         NaN           NaN            NaN         NaN         NaN  \n",
       "3         NaN           NaN            NaN         NaN         NaN  \n",
       "4         NaN           NaN            NaN         NaN         NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('/Users/dejmen/desktop/ironhack/week2/day1/GSAF5.xls')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e088dc5c-860e-4e7e-a15e-1e489e757eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('GSAF5.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ab1c16-703a-4901-bf65-45c4dd99071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249e5b13-5ef1-4343-aa5a-14b287cdc40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Type\"].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b210af4-b94b-4932-823a-0258188d9cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Injury\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4063a9-88f1-45c4-bbc4-597614dc870e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum_of_missing_values: 3526\n",
      "\n",
      "Percentage of missing values: 50.23%\n",
      "\n",
      "Huge format variations: see the last 50 value counts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Time\n",
       "14h21                             1\n",
       "15h53                             1\n",
       "\"Just before 11h00\"               1\n",
       "11h115                            1\n",
       "12h39                             1\n",
       "                                  1\n",
       "N                                 1\n",
       "Just before sundown               1\n",
       "22h30                             1\n",
       "11h30                             1\n",
       "06h10                             1\n",
       "Between 05h00 and 08h00           1\n",
       "07h08                             1\n",
       "17h00 or 17h40                    1\n",
       ">08h00                            1\n",
       "07h56                             1\n",
       "15h56                             1\n",
       "0830                              1\n",
       "17h46                             1\n",
       "21h50                             1\n",
       "19h00, Dusk                       1\n",
       "15h01                             1\n",
       "1000                              1\n",
       "10h44                             1\n",
       "13h19                             1\n",
       "Shortly before 12h00              1\n",
       "17h34                             1\n",
       "9h00                              1\n",
       "10h43                             1\n",
       "19h05                             1\n",
       "14h30 / 15h30                     1\n",
       "14h34                             1\n",
       "15h25                             1\n",
       "Morning                           1\n",
       "13h24                             1\n",
       "09h30 / 10h00                     1\n",
       "10h45-11h15                       1\n",
       "15h52                             1\n",
       "12.11 hrs                         1\n",
       "17h51                             1\n",
       "16h12                             1\n",
       "Sometime between 06h00 & 08hoo    1\n",
       "16h18                             1\n",
       "07h00 - 08h00                     1\n",
       "18h15-18h30                       1\n",
       "17h01                             1\n",
       "09h57                             1\n",
       "17h58                             1\n",
       "15h19                             1\n",
       "19h00-20h00                       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Challenges of Time feature: \n",
    "# 1 . alot of missing values\n",
    "print(f\"sum_of_missing_values: {df[\"Time\"].isnull().sum()}\")\n",
    "print()\n",
    "missing_percent = df[\"Time\"].isnull().sum() / len(df) * 100\n",
    "print(f\"Percentage of missing values: {missing_percent:.2f}%\")\n",
    "print ()\n",
    "print(f\"Huge format variations: see the last 50 value counts\") \n",
    "df[\"Time\"].value_counts().tail(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0ab2a309-b05f-427e-ad71-776b6bc7e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_time(value):\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "\n",
    "    value = str(value).strip().lower()\n",
    "\n",
    "    # Remove useless or unclear values\n",
    "    if value in [\"?\", \"am\", \"pm\", \"unknown\", \"not stated\", \"n/a\", \"na\"]:\n",
    "        return None\n",
    "\n",
    "    # Clean formats like \"after 1200hr\", \"11.30hr\", \"15.5\", etc.\n",
    "    match = re.search(r'(\\d{1,2})[h:.]?(\\d{2})?', value)\n",
    "\n",
    "    if match:\n",
    "        hour = match.group(1)\n",
    "        minute = match.group(2) if match.group(2) else \"00\"\n",
    "        return f\"{hour.zfill(2)}:{minute.zfill(2)}\"\n",
    "\n",
    "    # Keep known phrases like \"Morning\", \"Afternoon\", etc.\n",
    "    return value.title()\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3fb6d0-b495-4414-bbd5-8567cb2ac12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def precise_time_to_day_part(value):\n",
    "    if value is None:\n",
    "        return None  # Keep missing as None\n",
    "\n",
    "    # Known descriptive phrases to keep untouched\n",
    "    descriptive_parts = [\n",
    "        \"Early Morning\", \"Morning\", \"Midday\", \"Early Afternoon\",\n",
    "        \"Late Afternoon\", \"Afternoon\", \"Evening\", \"Dusk\",\n",
    "        \"Night\", \"Late Night\"\n",
    "    ]\n",
    "    \n",
    "    if isinstance(value, str) and value.title() in descriptive_parts:\n",
    "        return value.title()\n",
    "\n",
    "    try:\n",
    "        # Try parsing standard time like \"14:30\"\n",
    "        time = datetime.strptime(value, \"%H:%M\").time()\n",
    "        hour = time.hour\n",
    "        minute = time.minute\n",
    "\n",
    "        if 5 <= hour < 8:\n",
    "            return \"Early Morning\"\n",
    "        elif 8 <= hour < 12:\n",
    "            return \"Morning\"\n",
    "        elif hour == 12 and minute == 0:\n",
    "            return \"Midday\"\n",
    "        elif 12 <= hour < 15:\n",
    "            return \"Early Afternoon\"\n",
    "        elif 15 <= hour < 17:\n",
    "            return \"Late Afternoon\"\n",
    "        elif 17 <= hour < 19:\n",
    "            return \"Evening\"\n",
    "        elif 19 <= hour < 20:\n",
    "            return \"Dusk\"\n",
    "        elif 20 <= hour < 24:\n",
    "            return \"Night\"\n",
    "        else:  # 00:00 to before 5:00\n",
    "            return \"Late Night\"\n",
    "    except:\n",
    "        return None  # Unrecognized values go to None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bbb2c4-db47-4ede-b4dc-d60533f0131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cleaned_Time'] = df['Time'].apply(clean_time)\n",
    "df['Day_Part'] = df['Cleaned_Time'].apply(precise_time_to_day_part)\n",
    "print(df['Day_Part'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab64f08-1347-452c-be50-612b924ce354",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cleaned_Time'].head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ff2da-df19-4a0e-9633-9ff01144a18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Day_Part'].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750145cb-cfd0-4967-9d9f-d42513b71c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Date\"].value_counts().tail(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d908ec7-bb8c-4372-837d-6b4ce1a5f150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_date(date):\n",
    "    date = str(date)\n",
    "    \n",
    "    # Remove known unwanted words\n",
    "    cleaned_date = re.sub(r'\\b(Reported|Early|Before|No date|No Date)\\b', '', date, flags=re.IGNORECASE)\n",
    "    cleaned_date = re.sub(r'[^0-9a-zA-Z\\-/ :]', '', cleaned_date).strip()\n",
    "\n",
    "    # Try known formats first\n",
    "    for fmt in (\"%d-%b-%Y\", \"%d %b-%Y\", \"%Y-%m-%d %H:%M:%S\", \"%d-%m-%Y\", \"%Y-%m-%d\"):\n",
    "        try:\n",
    "            return pd.to_datetime(cleaned_date, format=fmt, errors='raise')\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # Fall back to automatic parsing (dayfirst off for ISO formats)\n",
    "    return pd.to_datetime(cleaned_date, errors='coerce', dayfirst=False)\n",
    "df['Cleaned_Date'] = df['Date'].apply(clean_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce3023c-199b-46ef-ab7f-414c94509873",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cleaned_Date'].tail(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d69a4fa-6e3f-41fc-9835-93cfe950c356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season(date):\n",
    "    if pd.isna(date):\n",
    "        return \"No Date\"\n",
    "    \n",
    "    month = date.month\n",
    "    if month in [3, 4, 5]:\n",
    "        return \"Spring\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"Summer\"\n",
    "    elif month in [9, 10, 11]:\n",
    "        return \"Autumn\"\n",
    "    elif month in [12, 1, 2]:\n",
    "        return \"Winter\"\n",
    "    \n",
    "    return \"No Date\"\n",
    "\n",
    "# Apply season mapping\n",
    "df['Season'] = df['Cleaned_Date'].apply(get_season)\n",
    "\n",
    "print(df[['Date', 'Cleaned_Date', 'Season']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afb351f-b3c4-4367-bb66-22be6298b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb1ffa-b371-4a10-87dd-dd4d27bc37fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df[\"Injury\"].value_counts().to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957156ce-d953-4929-8dd7-d98d398c9773",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df[\"Injury\"].value_counts()\n",
    "\n",
    "# Filter where count is exactly 1\n",
    "single_occurrences = counts[counts == 1]\n",
    "\n",
    "# Show how many have count = 1\n",
    "print(f\"Number of unique injuries that appear only once: {len(single_occurrences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fda8ea-171c-496b-902f-3b701d770597",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Injury_clean\"] = (\n",
    "    df[\"Injury\"]\n",
    "    .str.lower()\n",
    "    .str.strip()\n",
    "    .str.replace(r'[^a-z\\s]', '', regex=True)  # Remove non-letter characters\n",
    ")\n",
    "\n",
    "# Quick grouping preview\n",
    "print(df[\"Injury_clean\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caef37a1-edec-4485-81ce-dd525c0f428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_injury(text):\n",
    "    if pd.isna(text):\n",
    "        return \"unknown\"\n",
    "    text = text.lower()\n",
    "    if \"fatal\" in text:\n",
    "        return \"fatal\"\n",
    "    elif \"foot\" in text:\n",
    "        return \"foot injury\"\n",
    "    elif \"leg\" in text:\n",
    "        return \"leg injury\"\n",
    "    elif \"hand\" in text:\n",
    "        return \"hand injury\"\n",
    "    elif \"no injury\" in text:\n",
    "        return \"no injury\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "df[\"Injury_grouped\"] = df[\"Injury\"].apply(simplify_injury)\n",
    "print(df[\"Injury_grouped\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31615558-3fbe-4c81-9bc1-f17158c903b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"Injury_grouped\"].value_counts().to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dc19bd-7170-4c23-a4b5-e02c86167c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                 # HIPOLITO PART\n",
    "\n",
    "#CLEAN THE FATAL COlUMN:\n",
    "\n",
    "\n",
    "# Check the unique values in the column 'Fatal Y/N':\n",
    "print(df['Fatal Y/N'].dropna().apply(lambda x: repr(x)).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba6de5-8351-4d8d-9be3-5dd461c85607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change to string and eliminate spaces\n",
    "df['Fatal Y/N'] = df['Fatal Y/N'].astype(str).str.strip().str.upper()\n",
    "#Check for valid values\n",
    "valid_values = {'Y': 'Y', 'N': 'N'}\n",
    "#Put all the good values the rest will be NaN\n",
    "df['Fatal Y/N'] = df['Fatal Y/N'].map(valid_values)\n",
    "#Result\n",
    "print(df['Fatal Y/N'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b7feea-d90c-4c39-9747-bf11210e3589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of values (Y, N)\n",
    "print(df['Fatal Y/N'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f754035-34b2-456c-a02f-e26d978d036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique values in the column 'Country'\n",
    "print(df['Country'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb10faae-65a7-47c3-932e-041bdcd5323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All in mayus and eliminate spaces\n",
    "df['Country'] = df['Country'].str.strip().str.upper()\n",
    "print(df['Country'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a50f760-c50f-4b3e-bee7-93224179762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING DATA OF COUNTRIES\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "country_corrections = {\n",
    "    # Correccions Ortografics\n",
    "    'COLUMBIA': 'COLOMBIA',\n",
    "    'TRINIDAD & TOBAGO': 'TRINIDAD AND TOBAGO',\n",
    "    'MALDIVE ISLANDS': 'MALDIVES',\n",
    "    'UNITED ARAB EMIRATES (UAE)': 'UNITED ARAB EMIRATES',\n",
    "    'ST. MARTIN': 'ST MARTIN',\n",
    "    'ST. MAARTIN': 'ST MARTIN',\n",
    "    'TRINIDAD': 'TRINIDAD AND TOBAGO',\n",
    "\n",
    "    # Agrupations\n",
    "    'ENGLAND': 'UK',\n",
    "    'SCOTLAND': 'UK',\n",
    "    'UNITED KINGDOM': 'UK',\n",
    "    'BRITISH ISLES': 'UK',\n",
    "    'BRITISH WEST INDIES': 'UK',\n",
    "    'BRITISH VIRGIN ISLANDS': 'UK',\n",
    "\n",
    "    # Ocean y region not usefull\n",
    "    'PACIFIC OCEAN': 'OTHER',\n",
    "    'ATLANTIC OCEAN': 'OTHER',\n",
    "    'INDIAN OCEAN': 'OTHER',\n",
    "    'SOUTH PACIFIC OCEAN': 'OTHER',\n",
    "    'CARIBBEAN SEA': 'OTHER',\n",
    "    'OCEAN': 'OTHER',\n",
    "    'GULF OF ADEN': 'OTHER',\n",
    "    'MID-PACIFC OCEAN': 'OTHER',\n",
    "    'NORTH ATLANTIC OCEAN': 'OTHER',\n",
    "    'RED SEA': 'OTHER',\n",
    "    'RED SEA / INDIAN OCEAN': 'OTHER',\n",
    "    'NORTH PACIFIC OCEAN': 'OTHER',\n",
    "    'CENTRAL PACIFIC': 'OTHER',\n",
    "\n",
    "    # Some other mistakes â†’ agrupar\n",
    "    'DIEGO GARCIA': 'OTHER',\n",
    "    'JOHNSTON ISLAND': 'OTHER',\n",
    "    'ADMIRALTY ISLANDS': 'OTHER',\n",
    "    'MID ATLANTIC OCEAN': 'OTHER',\n",
    "    'UNKNOWN': 'OTHER',\n",
    "    'AFRICA': 'OTHER',\n",
    "    'ASIA?': 'OTHER',\n",
    "    'SUDAN?': 'SUDAN',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a757f459-86a8-4408-99f8-ac67f415af62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the country correction on or columns\n",
    "\n",
    "def clean_column_country(df, column='Country'):\n",
    "    # All mayus\n",
    "    df[column] = df[column].str.strip().str.upper()\n",
    "    #use the country_corrections to filter the column\n",
    "    df[column] = df[column].replace(country_corrections)\n",
    "    return df\n",
    "\n",
    "df = clean_column_country(df, column='Country')\n",
    "\n",
    "print(sorted(df['Country'].dropna().unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71be513e-41b4-4a43-94ae-a9e329b79716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 countries with more sharks attacts:\n",
    "top_10_paises = df['Country'].value_counts().head(10)\n",
    "print(top_10_paises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f07c6991-9b4c-401a-bcae-3e3f0831cbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 activities plus others:\n",
      "Surfing             1133\n",
      "Swimming             997\n",
      "Fishing              490\n",
      "Spearfishing         388\n",
      "Wading               177\n",
      "Bathing              164\n",
      "Diving               147\n",
      "Snorkeling           132\n",
      "Standing             113\n",
      "Scuba diving          84\n",
      "Other activities    2610\n",
      "dtype: int64\n",
      "\n",
      "Formatted table:\n",
      "|                  |    0 |\n",
      "|:-----------------|-----:|\n",
      "| Surfing          | 1133 |\n",
      "| Swimming         |  997 |\n",
      "| Fishing          |  490 |\n",
      "| Spearfishing     |  388 |\n",
      "| Wading           |  177 |\n",
      "| Bathing          |  164 |\n",
      "| Diving           |  147 |\n",
      "| Snorkeling       |  132 |\n",
      "| Standing         |  113 |\n",
      "| Scuba diving     |   84 |\n",
      "| Other activities | 2610 |\n"
     ]
    }
   ],
   "source": [
    "activity_counts = df['Activity'].value_counts()\n",
    "\n",
    "# Get top 10\n",
    "top_10 = activity_counts.head(10)\n",
    "\n",
    "# Calculate sum of all other activities\n",
    "other_count = activity_counts[10:].sum()\n",
    "\n",
    "# Create a new series with \"Other activities\" included\n",
    "top_10_with_other = pd.concat([\n",
    "    top_10, \n",
    "    pd.Series({'Other activities': other_count})\n",
    "])\n",
    "\n",
    "print(\"Top 10 activities plus others:\")\n",
    "print(top_10_with_other)\n",
    "\n",
    "print(\"\\nFormatted table:\")\n",
    "print(top_10_with_other.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "997f196a-976e-4527-8308-9eb03f0892b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activity Statistics (Top 10 + Others):\n",
      "| Activity         |   Count |   Percentage |\n",
      "|:-----------------|--------:|-------------:|\n",
      "| surfing          |    1138 |         17.7 |\n",
      "| swimming         |    1044 |         16.2 |\n",
      "| fishing          |     506 |          7.9 |\n",
      "| spearfishing     |     396 |          6.2 |\n",
      "| wading           |     177 |          2.8 |\n",
      "| bathing          |     167 |          2.6 |\n",
      "| diving           |     150 |          2.3 |\n",
      "| snorkeling       |     133 |          2.1 |\n",
      "| standing         |     115 |          1.8 |\n",
      "| scuba diving     |     104 |          1.6 |\n",
      "| Other activities |    2505 |         38.9 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/74/2xg39v_50tv7vj24jw0r0b800000gn/T/ipykernel_28325/3083507710.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  top_activities['Percentage'] = (top_activities['Count'] / total * 100).round(1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_activity_stats(df, activity_column='Activity', top_n=10):\n",
    "\n",
    "    # Make copy to avoid modifying original DataFrame\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Standard cleaning\n",
    "    df[activity_column] = (\n",
    "        df[activity_column]\n",
    "        .str.strip()  # Remove whitespace\n",
    "        .str.lower()  # Convert to lowercase\n",
    "    )\n",
    "    \n",
    "    # Common activity replacements (customize as needed)\n",
    "    activity_replacements = {\n",
    "        'swim': 'swimming',\n",
    "        'bike': 'cycling',\n",
    "        'bicycle': 'cycling',\n",
    "        'football': 'soccer',\n",
    "        'bball': 'basketball',\n",
    "        'hoops': 'basketball',\n",
    "        # Add more as needed for your dataset\n",
    "    }\n",
    "    \n",
    "    df[activity_column] = df[activity_column].replace(activity_replacements)\n",
    "    \n",
    "    # Get activity counts\n",
    "    activity_counts = df[activity_column].value_counts().reset_index()\n",
    "    activity_counts.columns = ['Activity', 'Count']\n",
    "    total = activity_counts['Count'].sum()\n",
    "    \n",
    "    # Separate top N and others\n",
    "    top_activities = activity_counts.head(top_n)\n",
    "    other_count = activity_counts['Count'][top_n:].sum()\n",
    "    \n",
    "    # Create \"Other\" row\n",
    "    other_row = pd.DataFrame({\n",
    "        'Activity': ['Other activities'],\n",
    "        'Count': [other_count],\n",
    "        'Percentage': [(other_count / total * 100).round(1)]\n",
    "    })\n",
    "    \n",
    "    # Calculate percentages for top activities\n",
    "    top_activities['Percentage'] = (top_activities['Count'] / total * 100).round(1)\n",
    "    \n",
    "    # Combine results\n",
    "    result = pd.concat([top_activities, other_row], ignore_index=True)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Usage example:\n",
    "activity_stats = get_activity_stats(df, top_n=10)\n",
    "\n",
    "print(\"Activity Statistics (Top 10 + Others):\")\n",
    "print(activity_stats.to_markdown(index=False, floatfmt=\".1f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a99821f-bfc3-4697-8884-df8fc19f479e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
